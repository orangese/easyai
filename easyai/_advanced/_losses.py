"""

"easyai._advanced._losses"

Advanced keras losses, activity regularizers, and support. Not for use by easyai users-- does not use easyai API.

"""

from keras import backend as K

from easyai.core import *


# SUPPORT FOR CUSTOM LOSS
class Evaluator(object):
    """Class used for custom loss and gradient functions. Should be used in conjunction with scipy.optimize.[whatever].
    """

    def __init__(self, obj):
        self.obj = obj
        assert hasattr(obj, "loss_and_grads"), "obj must have loss_and_grads function"
        self.reset()

    def f_loss(self, img):
        loss, grads = self.obj.loss_and_grads(img)
        self.loss = loss
        self.grads = grads
        return self.loss

    def f_grads(self, img):
        grads = np.copy(self.grads)
        self.reset()
        return grads

    def reset(self):
        self.loss = None
        self.grads = None


# REGULARIZERS FOR NST
class StyleRegularizer(keras.regularizers.Regularizer):
    """Style regularizer for fast NST."""

    def __init__(self, style_img, weight):
        self.weight = weight
        self.style_gram = self.gram_matrix(style_img)
        self.uses_learning_phase = False
        super(StyleRegularizer, self).__init__()

    def __call__(self, x):
        gram_x = self.gram_matrix(x.output[0])
        return K.sum(K.square(gram_x - self.style_gram))
        # x.output[0] is generated by network, x.output[1] is the true label

    def gram_matrix(self, a):
        norm_term = self.weight / K.prod(K.cast(K.shape(a), dtype="float32"))
        a = K.batch_flatten(K.permute_dimensions(a, (2, 0, 1)))
        return norm_term * K.dot(a, K.transpose(a))


class ContentRegularizer(keras.regularizers.Regularizer):
    """Content regularizer for fast NST."""

    # FIXME: this breaks fast neural style transfer! when optimizing for content loss, square noise is generated

    def __init__(self, weight):
        self.weight = weight
        self.uses_learning_phase = False
        super(ContentRegularizer, self).__init__()

    def __call__(self, x):
        norm_term = self.weight / K.prod(K.cast(K.shape(x.output[0]), dtype="float32"))
        return norm_term * K.sum(K.square(x.output[1] - x.output[0]))

class TVRegularizer(keras.regularizers.Regularizer):

    def __init__(self, weight):
        self.weight = weight
        self.uses_learning_phase = False
        super(TVRegularizer, self).__init__()

    def __call__(self, x):
        shape = K.shape(x.output)
        num_rows, num_cols, channels = shape[0], shape[1], shape[2]
        # tensors are not iterable unless eager execution is enabled
        a = K.square(x.output[:, :num_rows - 1, :num_cols - 1, :] - x.output[:, 1:, :num_cols - 1, :])
        b = K.square(x.output[:, :num_rows - 1, :num_cols - 1, :] - x.output[:, :num_rows - 1, 1:, :])

        return self.weight * K.sum(K.pow(a + b, 1.25))
